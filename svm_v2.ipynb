{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from sklearn import svm, naive_bayes\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_precessing(df):\n",
    "    # text cleaning and preparation\n",
    "    df['content'] = df['article_words'].str.replace(',', ' ')\n",
    "    # label coding\n",
    "    topic_codes = {\n",
    "        'IRRELEVANT': 0,\n",
    "        'ARTS CULTURE ENTERTAINMENT': 1,\n",
    "        'BIOGRAPHIES PERSONALITIES PEOPLE': 2,\n",
    "        'DEFENCE': 3,\n",
    "        'DOMESTIC MARKETS': 4,\n",
    "        'FOREX MARKETS': 5,\n",
    "        'HEALTH': 6,\n",
    "        'MONEY MARKETS': 7,\n",
    "        'SCIENCE AND TECHNOLOGY': 8,\n",
    "        'SHARE LISTINGS': 9,\n",
    "        'SPORTS': 10 \n",
    "    }\n",
    "    df['topic_code'] = df['topic']\n",
    "    df = df.replace({'topic_code':topic_codes})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(df):\n",
    "    for i in range(len(df)):\n",
    "        a=df.loc[i]\n",
    "        d=pd.DataFrame(a).T\n",
    "        if df.at[i,'topic'] != \"IRRELEVANT\":\n",
    "            df=df.append([d]*2)  \n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv')\n",
    "df_train = pd.read_csv('training.csv')\n",
    "\n",
    "# with data augment\n",
    "train_content_with_dataAugment = data_precessing(data_augment(df_train))['content']\n",
    "train_with_dataAugment = data_precessing(data_augment(df_train))['topic_code']\n",
    "\n",
    "X_train_content_with_dataAugment, X_test_content_with_dataAugment, y_train_with_dataAugment, y_test_with_dataAugment = train_test_split(train_content_with_dataAugment, \n",
    "                                                    train_with_dataAugment, \n",
    "                                                    test_size=0.0526, \n",
    "                                                    random_state=8)\n",
    "# without data augment\n",
    "train_content_without_dataAugment = data_precessing(df_train)['content']\n",
    "train_without_dataAugment = data_precessing(df_train)['topic_code']\n",
    "X_train_content_without_dataAugment, X_test_content_without_dataAugment, y_train_without_dataAugment, y_test_without_dataAugment = train_test_split(train_content_without_dataAugment, \n",
    "                                                    train_without_dataAugment, \n",
    "                                                    test_size=0.0526, \n",
    "                                                    random_state=8)\n",
    "# test data\n",
    "X_test_content = data_precessing(df_test)['content']\n",
    "y_test = data_precessing(df_test)['topic_code']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_representation(content, sign):\n",
    "    # text representation\n",
    "    # Parameter election\n",
    "    ngram_range = (1,1)\n",
    "    min_df = 0.04\n",
    "    max_df = 0.3\n",
    "    max_features = 400\n",
    "    # TF_IDF\n",
    "    tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                            ngram_range=ngram_range,\n",
    "                            stop_words=None,\n",
    "                            lowercase=False,\n",
    "                            max_df=max_df,\n",
    "                            min_df=min_df,\n",
    "                            max_features=max_features,\n",
    "                            sublinear_tf=True)\n",
    "    \n",
    "    if sign == 0:\n",
    "        tf_fit = tfidf.fit_transform(content)\n",
    "    else:\n",
    "        tf_fit = tfidf.transform(content)\n",
    "    features = tf_fit.toarray()\n",
    "    return features\n",
    "# X_train = text_representation(X_train_content,0)\n",
    "# X_test = text_representation(X_test_content,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 35261)\n",
      "(500, 34839)\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer_with = CountVectorizer()\n",
    "count_vectorizer_without = CountVectorizer()\n",
    "# with data augment\n",
    "X_train_with_dataAugment = count_vectorizer_with.fit_transform(X_train_content_with_dataAugment)\n",
    "X_test_with_dataAugment = count_vectorizer_with.transform(X_test_content_with_dataAugment)\n",
    "\n",
    "# without data augment\n",
    "X_train_without_dataAugment = count_vectorizer_without.fit_transform(X_train_content_without_dataAugment)\n",
    "X_test_without_dataAugment = count_vectorizer_without.transform(X_test_content_without_dataAugment)\n",
    "\n",
    "# test data\n",
    "X_test_with = count_vectorizer_with.transform(X_test_content)\n",
    "X_test_without = count_vectorizer_without.transform(X_test_content)\n",
    "print(X_test_with.shape)\n",
    "print(X_test_without.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy with data augment is:  0.941985579589573\n",
      "The test accuracy with data augment is:  0.9001996007984032\n",
      "The training accuracy without data augment is:  0.897\n",
      "The test accuracy without data augment is:  0.748\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "# best_svc = svm.SVC(C=0.1, gamma=1, kernel='rbf', max_iter=-1, probability=True, random_state=8)\n",
    "with_dataAugment_svm = svm.SVC()\n",
    "without_dataAugment_svm = svm.SVC()\n",
    "# nb = naive_bayes.MultinomialNB()\n",
    "\n",
    "# best_svc.fit(X_train, y_train)\n",
    "with_dataAugment_svm.fit(X_train_with_dataAugment,y_train_with_dataAugment)\n",
    "without_dataAugment_svm.fit(X_train_without_dataAugment,y_train_without_dataAugment)\n",
    "\n",
    "# nb.fit(X_train, y_train)\n",
    "# train_pred = nb.predict(X_train)\n",
    "# test_pred = nb.predict(X_test)\n",
    "\n",
    "\n",
    "# with data augment\n",
    "train_pred_with_dataAugment = with_dataAugment_svm.predict(X_train_with_dataAugment)\n",
    "test_pred_with_dataAugment = with_dataAugment_svm.predict(X_test_with_dataAugment)\n",
    "\n",
    "# without data augment\n",
    "train_pred_without_dataAugment = without_dataAugment_svm.predict(X_train_without_dataAugment)\n",
    "test_pred_without_dataAugment = without_dataAugment_svm.predict(X_test_without_dataAugment)\n",
    "\n",
    "\n",
    "# With data augment accuracy\n",
    "print(\"The training accuracy with data augment is: \", accuracy_score(y_train_with_dataAugment,train_pred_with_dataAugment))\n",
    "print(\"The test accuracy with data augment is: \", accuracy_score(y_test_with_dataAugment,test_pred_with_dataAugment))\n",
    "\n",
    "\n",
    "# Without data augment accuracy\n",
    "print(\"The training accuracy without data augment is: \", accuracy_score(y_train_without_dataAugment,train_pred_without_dataAugment))\n",
    "print(\"The test accuracy without data augment is: \", accuracy_score(y_test_without_dataAugment,test_pred_without_dataAugment))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_report_with_dataAugment: \n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "                      IRRELEVANT       0.98      0.93      0.96      4466\n",
      "      ARTS CULTURE ENTERTAINMENT       0.99      0.98      0.98       331\n",
      "BIOGRAPHIES PERSONALITIES PEOPLE       0.99      0.99      0.99       470\n",
      "                         DEFENCE       0.98      0.99      0.99       736\n",
      "                DOMESTIC MARKETS       0.98      0.99      0.98       378\n",
      "                   FOREX MARKETS       0.89      0.80      0.85      2409\n",
      "                          HEALTH       0.99      0.99      0.99       516\n",
      "                   MONEY MARKETS       0.88      0.95      0.91      4761\n",
      "          SCIENCE AND TECHNOLOGY       1.00      1.00      1.00       203\n",
      "                  SHARE LISTINGS       0.96      0.98      0.97       630\n",
      "                          SPORTS       0.98      1.00      0.99      3130\n",
      "\n",
      "                        accuracy                           0.94     18030\n",
      "                       macro avg       0.97      0.96      0.96     18030\n",
      "                    weighted avg       0.94      0.94      0.94     18030\n",
      "\n",
      "test_report_with_dataAugment: \n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "                      IRRELEVANT       0.96      0.88      0.92       268\n",
      "      ARTS CULTURE ENTERTAINMENT       1.00      0.90      0.95        20\n",
      "BIOGRAPHIES PERSONALITIES PEOPLE       1.00      1.00      1.00        31\n",
      "                         DEFENCE       1.00      0.97      0.99        38\n",
      "                DOMESTIC MARKETS       0.87      0.95      0.91        21\n",
      "                   FOREX MARKETS       0.84      0.64      0.73       126\n",
      "                          HEALTH       0.89      0.94      0.91        33\n",
      "                   MONEY MARKETS       0.81      0.94      0.87       258\n",
      "          SCIENCE AND TECHNOLOGY       0.88      1.00      0.93         7\n",
      "                  SHARE LISTINGS       0.85      0.96      0.90        24\n",
      "                          SPORTS       0.97      1.00      0.99       176\n",
      "\n",
      "                        accuracy                           0.90      1002\n",
      "                       macro avg       0.91      0.93      0.92      1002\n",
      "                    weighted avg       0.90      0.90      0.90      1002\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_name = ['IRRELEVANT',\n",
    "    'ARTS CULTURE ENTERTAINMENT',\n",
    "    'BIOGRAPHIES PERSONALITIES PEOPLE',\n",
    "    'DEFENCE',\n",
    "    'DOMESTIC MARKETS',\n",
    "    'FOREX MARKETS',\n",
    "    'HEALTH',\n",
    "    'MONEY MARKETS',\n",
    "    'SCIENCE AND TECHNOLOGY',\n",
    "    'SHARE LISTINGS',\n",
    "    'SPORTS'\n",
    "]\n",
    "# with data augment\n",
    "print(\"train_report_with_dataAugment: \")\n",
    "train_report_with_dataAugment = classification_report(y_train_with_dataAugment, train_pred_with_dataAugment,target_names=features_name)\n",
    "print(train_report_with_dataAugment)\n",
    "\n",
    "print(\"test_report_with_dataAugment: \")\n",
    "test_report_with_dataAugment = classification_report(y_test_with_dataAugment, test_pred_with_dataAugment,target_names=features_name)\n",
    "print(test_report_with_dataAugment)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_report_without_dataAugment: \n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "                      IRRELEVANT       0.92      0.95      0.94      4471\n",
      "      ARTS CULTURE ENTERTAINMENT       0.97      0.63      0.77       111\n",
      "BIOGRAPHIES PERSONALITIES PEOPLE       0.99      0.74      0.85       159\n",
      "                         DEFENCE       0.98      0.88      0.93       242\n",
      "                DOMESTIC MARKETS       0.94      0.71      0.81       129\n",
      "                   FOREX MARKETS       0.86      0.61      0.71       799\n",
      "                          HEALTH       1.00      0.80      0.89       168\n",
      "                   MONEY MARKETS       0.78      0.92      0.84      1596\n",
      "          SCIENCE AND TECHNOLOGY       1.00      0.72      0.84        68\n",
      "                  SHARE LISTINGS       0.97      0.71      0.82       210\n",
      "                          SPORTS       0.95      1.00      0.97      1047\n",
      "\n",
      "                        accuracy                           0.90      9000\n",
      "                       macro avg       0.94      0.79      0.85      9000\n",
      "                    weighted avg       0.90      0.90      0.89      9000\n",
      "\n",
      "test_report_without_dataAugment: \n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "                      IRRELEVANT       0.79      0.92      0.85       263\n",
      "      ARTS CULTURE ENTERTAINMENT       1.00      0.17      0.29         6\n",
      "BIOGRAPHIES PERSONALITIES PEOPLE       0.00      0.00      0.00         8\n",
      "                         DEFENCE       0.80      0.25      0.38        16\n",
      "                DOMESTIC MARKETS       0.00      0.00      0.00         4\n",
      "                   FOREX MARKETS       0.50      0.26      0.34        46\n",
      "                          HEALTH       0.67      0.13      0.22        15\n",
      "                   MONEY MARKETS       0.56      0.73      0.63        77\n",
      "          SCIENCE AND TECHNOLOGY       1.00      0.50      0.67         2\n",
      "                  SHARE LISTINGS       0.71      0.62      0.67         8\n",
      "                          SPORTS       0.96      0.93      0.94        55\n",
      "\n",
      "                        accuracy                           0.75       500\n",
      "                       macro avg       0.64      0.41      0.45       500\n",
      "                    weighted avg       0.73      0.75      0.72       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# without data augment\n",
    "print(\"train_report_without_dataAugment: \")\n",
    "train_report_without_dataAugment = classification_report(y_train_without_dataAugment, train_pred_without_dataAugment,target_names=features_name)\n",
    "print(train_report_without_dataAugment)\n",
    "\n",
    "print(\"test_report_without_dataAugment: \")\n",
    "test_report_with_dataAugment = classification_report(y_test_without_dataAugment, test_pred_without_dataAugment,target_names=features_name)\n",
    "print(test_report_with_dataAugment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy with data augment is:  0.762\n",
      "test_report_with data augment: \n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "                      IRRELEVANT       0.84      0.88      0.86       266\n",
      "      ARTS CULTURE ENTERTAINMENT       0.50      0.33      0.40         3\n",
      "BIOGRAPHIES PERSONALITIES PEOPLE       1.00      0.33      0.50        15\n",
      "                         DEFENCE       1.00      0.38      0.56        13\n",
      "                DOMESTIC MARKETS       0.33      0.50      0.40         2\n",
      "                   FOREX MARKETS       0.50      0.35      0.41        48\n",
      "                          HEALTH       0.67      0.43      0.52        14\n",
      "                   MONEY MARKETS       0.53      0.72      0.61        69\n",
      "          SCIENCE AND TECHNOLOGY       0.00      0.00      0.00         3\n",
      "                  SHARE LISTINGS       0.71      0.71      0.71         7\n",
      "                          SPORTS       0.95      0.93      0.94        60\n",
      "\n",
      "                        accuracy                           0.76       500\n",
      "                       macro avg       0.64      0.51      0.54       500\n",
      "                    weighted avg       0.77      0.76      0.75       500\n",
      "\n",
      "The test accuracy without is:  0.738\n",
      "test_report_without: \n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "                      IRRELEVANT       0.78      0.91      0.84       266\n",
      "      ARTS CULTURE ENTERTAINMENT       0.00      0.00      0.00         3\n",
      "BIOGRAPHIES PERSONALITIES PEOPLE       0.00      0.00      0.00        15\n",
      "                         DEFENCE       1.00      0.38      0.56        13\n",
      "                DOMESTIC MARKETS       0.50      0.50      0.50         2\n",
      "                   FOREX MARKETS       0.48      0.25      0.33        48\n",
      "                          HEALTH       0.67      0.14      0.24        14\n",
      "                   MONEY MARKETS       0.52      0.70      0.60        69\n",
      "          SCIENCE AND TECHNOLOGY       0.00      0.00      0.00         3\n",
      "                  SHARE LISTINGS       0.67      0.29      0.40         7\n",
      "                          SPORTS       0.95      0.93      0.94        60\n",
      "\n",
      "                        accuracy                           0.74       500\n",
      "                       macro avg       0.51      0.37      0.40       500\n",
      "                    weighted avg       0.70      0.74      0.70       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_pred_with = with_dataAugment_svm.predict(X_test_with)\n",
    "print(\"The test accuracy with data augment is: \", accuracy_score(y_test,test_pred_with))\n",
    "print(\"test_report_with data augment: \")\n",
    "test_report_with = classification_report(y_test, test_pred_with, target_names=features_name)\n",
    "print(test_report_with)\n",
    "\n",
    "test_pred_without = without_dataAugment_svm.predict(X_test_without)\n",
    "print(\"The test accuracy without is: \", accuracy_score(y_test,test_pred_without))\n",
    "print(\"test_report_without: \")\n",
    "test_report_without = classification_report(y_test, test_pred_without, target_names=features_name)\n",
    "print(test_report_without)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  7  0  0  5  0 10  7  0  0  0 10 10  0  7  0  9  0 10  0  0  0  0\n",
      "  7  2  0  0  5  5  7  0  0  7  0 10  0  0  7  0 10  7  0  0  0  0  7  5\n",
      "  0  7  5  0  7  7  7  0  0  0  3  7  0  9  0  0  5  0  0 10 10  7  7  5\n",
      " 10 10  0  3  5  0  0 10  0 10  7  5  0  7  7  5  0  0  0  0  0  7  0 10\n",
      "  0  0  7  7  9  7  0  0  0  0  0 10  0 10  0  0  0  0  0  0  8  7  0 10\n",
      "  6  0  0  0  7  0  7  0  0 10  0  5  0  7  0  0  0  7  0  4  7  0  0  0\n",
      "  0  0  7  0  0  7  0  0 10  0  0 10 10  0  7  0  0 10 10  0  7  9  9  0\n",
      "  0  3  5  7  0  0  0  7  7  7  0  7  0  5  7  0  0  0  0  0  0  0  7  0\n",
      "  7 10 10  7  0  0  0  0 10  5  1  7  0  0  7  7  0  0  7  0  0  0  9  0\n",
      "  0  5  0  7  0  0  7  0  7  0  0  0  5  0  0  0  7  7  0 10  7 10  0  7\n",
      "  0  0  5  0  0  7  0  5  0  0  0 10  0 10  7  0 10  0  0 10  7  0 10  0\n",
      "  7  7  0  0  7  3  0  5  3  0  0  7  0  0  0  0  0 10  0  0  0  5 10  0\n",
      "  1  0 10  0  0  0  0  0  0  5  0 10 10  0  0  0  7  5  6  7 10  6 10  0\n",
      " 10  0  0  7 10 10  0  7 10  7  7  7  0  5  0  7  0  0  0 10  6  0  7 10\n",
      "  7  0  0  7  0  0  0  0  7  0  0 10 10  0  5  5  7  0  5  0  0 10  0  7\n",
      "  0 10  7  0  0  7  0  0  7  0  7  0  6  0  5  0  7  2  0  0  0  0  7  0\n",
      "  0 10  0  0  5  0  0  0  0  5  0  0  0  7  0  0  7  5  0  0  0  0  0  0\n",
      "  0  0  6  0  0  0  0  7  0  0 10 10  0 10  0  7  0  6  0  0  6  0 10  0\n",
      "  2  0  0  0  6  7  7  2  0 10  0  5  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  7  0  7  0  5 10  7  0  7  0  0  0  0  9  0  0  7  0  5  0 10  0\n",
      " 10  0  0  7  0  5  0  2  4  7  0 10  0  4  7  0  0  7  0  0]\n"
     ]
    }
   ],
   "source": [
    "# use data with data augment to predict\n",
    "print(test_pred_with)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\t#nbc means naive bayes classifier\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from scipy.stats import sem\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data\n",
    "df_train = pd.read_csv('training.csv')\n",
    "train_set = pd.DataFrame(columns=['words', 'topic'])\n",
    "train_set['words'] = df_train['article_words'].str.replace(',',' ')\n",
    "train_set['topic'] = df_train['topic']\n",
    "\n",
    "# split the data to train set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_set['words'], train_set['topic'], test_size = 0.0526, random_state = 1)\n",
    "\n",
    "# encoder the y\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.fit_transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WordLevel tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word',token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(train_set['words'])\n",
    "xtrain_tfidf = tfidf_vect.transform(X_train)\n",
    "xtest_tfidf = tfidf_vect.transform(X_test)\n",
    "\n",
    "# NgramLevel tf-idf\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word',token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(train_set['words'])\n",
    "xtrain_tfidf_ngram = tfidf_vect_ngram.transform(X_train)\n",
    "xtest_tfidf_ngram = tfidf_vect_ngram.transform(X_test)\n",
    "\n",
    "# CharLevel tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(train_set['words'])\n",
    "xtrain_tfidf_ngram_chars = tfidf_vect_ngram_chars.transform(X_train)\n",
    "xtest_tfidf_ngram_chars = tfidf_vect_ngram_chars.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, WordLevel TF-IDF:  0.782\n",
      "LR, N-Gram Vectors TF-IDF:  0.752\n",
      "LR, CharLevel Vectors TF-IDF:  0.762\n"
     ]
    }
   ],
   "source": [
    "#  solver='lbfgs'\n",
    "\n",
    "# Linear classifier characterized by word-level TF-IDF vector\n",
    "lg1 = linear_model.LogisticRegressionCV(Cs=[1, 10, 100, 1000], class_weight=None, cv=5,\n",
    "           dual=False, fit_intercept=True, intercept_scaling=1.0,\n",
    "           max_iter=2000, multi_class='multinomial', n_jobs=1, penalty='l2',\n",
    "           random_state=None, refit=True, scoring='neg_log_loss',\n",
    "           solver='lbfgs', tol=0.0001, verbose=0).fit(xtrain_tfidf, y_train)\n",
    "predictions = lg1.predict(xtest_tfidf)\n",
    "accuracy = metrics.accuracy_score(predictions, y_test)\n",
    "print(\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Linear classifier characterized by multiple word-level TF-IDF vectors\n",
    "lg2 = linear_model.LogisticRegressionCV(Cs=[1, 10, 100, 1000], class_weight=None, cv=5,\n",
    "           dual=False, fit_intercept=True, intercept_scaling=1.0,\n",
    "           max_iter=2000, multi_class='multinomial', n_jobs=1, penalty='l2',\n",
    "           random_state=None, refit=True, scoring='neg_log_loss',\n",
    "           solver='lbfgs', tol=0.0001, verbose=0).fit(xtrain_tfidf_ngram, y_train)\n",
    "predictions = lg2.predict(xtest_tfidf_ngram)\n",
    "accuracy = metrics.accuracy_score(predictions, y_test)\n",
    "print(\"LR, N-Gram Vectors TF-IDF: \", accuracy)\n",
    "\n",
    "\n",
    "# Linear classifier characterized by part-of-speech TF-IDF vector\n",
    "lg3 = linear_model.LogisticRegressionCV(Cs=[1, 10, 100, 1000], class_weight=None, cv=5,\n",
    "           dual=False, fit_intercept=True, intercept_scaling=1.0,\n",
    "           max_iter=2000, multi_class='multinomial', n_jobs=1, penalty='l2',\n",
    "           random_state=None, refit=True, scoring='neg_log_loss',\n",
    "           solver='lbfgs', tol=0.0001, verbose=0).fit(xtrain_tfidf_ngram_chars, y_train)\n",
    "predictions = lg3.predict(xtest_tfidf_ngram_chars)\n",
    "accuracy = metrics.accuracy_score(predictions, y_test)\n",
    "print(\"LR, CharLevel Vectors TF-IDF: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, WordLevel TF-IDF:  0.782\n",
      "LR, N-Gram Vectors TF-IDF:  0.752\n",
      "LR, CharLevel Vectors TF-IDF:  0.762\n"
     ]
    }
   ],
   "source": [
    "#solver='newton-cg'\n",
    "\n",
    "# Linear classifier characterized by word-level TF-IDF vector\n",
    "lg_1 = linear_model.LogisticRegressionCV(Cs=[1, 10, 100, 1000], class_weight=None, cv=5,\n",
    "           dual=False, fit_intercept=True, intercept_scaling=1.0,\n",
    "           max_iter=2000, multi_class='multinomial', n_jobs=1, penalty='l2',\n",
    "           random_state=None, refit=True, scoring='neg_log_loss',\n",
    "           solver='newton-cg', tol=0.0001, verbose=0).fit(xtrain_tfidf, y_train)\n",
    "predictions = lg1.predict(xtest_tfidf)\n",
    "accuracy = metrics.accuracy_score(predictions, y_test)\n",
    "print(\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Linear classifier characterized by multiple word-level TF-IDF vectors\n",
    "lg_2 = linear_model.LogisticRegressionCV(Cs=[1, 10, 100, 1000], class_weight=None, cv=5,\n",
    "           dual=False, fit_intercept=True, intercept_scaling=1.0,\n",
    "           max_iter=2000, multi_class='multinomial', n_jobs=1, penalty='l2',\n",
    "           random_state=None, refit=True, scoring='neg_log_loss',\n",
    "           solver='newton-cg', tol=0.0001, verbose=0).fit(xtrain_tfidf_ngram, y_train)\n",
    "predictions = lg2.predict(xtest_tfidf_ngram)\n",
    "accuracy = metrics.accuracy_score(predictions, y_test)\n",
    "print(\"LR, N-Gram Vectors TF-IDF: \", accuracy)\n",
    "\n",
    "\n",
    "# Linear classifier characterized by part-of-speech TF-IDF vector\n",
    "lg_3 = linear_model.LogisticRegressionCV(Cs=[1, 10, 100, 1000], class_weight=None, cv=5,\n",
    "           dual=False, fit_intercept=True, intercept_scaling=1.0,\n",
    "           max_iter=2000, multi_class='multinomial', n_jobs=1, penalty='l2',\n",
    "           random_state=None, refit=True, scoring='neg_log_loss',\n",
    "           solver='newton-cg', tol=0.0001, verbose=0).fit(xtrain_tfidf_ngram_chars, y_train)\n",
    "predictions = lg3.predict(xtest_tfidf_ngram_chars)\n",
    "accuracy = metrics.accuracy_score(predictions, y_test)\n",
    "print(\"LR, CharLevel Vectors TF-IDF: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "      ARTS CULTURE ENTERTAINMENT       0.75      0.60      0.67         5\n",
      "BIOGRAPHIES PERSONALITIES PEOPLE       0.67      0.57      0.62         7\n",
      "                         DEFENCE       0.82      0.82      0.82        11\n",
      "                DOMESTIC MARKETS       0.33      0.17      0.22         6\n",
      "                   FOREX MARKETS       0.32      0.34      0.33        41\n",
      "                          HEALTH       0.50      0.44      0.47         9\n",
      "                       RRELEVANT       0.84      0.89      0.86       249\n",
      "                   MONEY MARKETS       0.60      0.58      0.59        99\n",
      "          SCIENCE AND TECHNOLOGY       0.75      1.00      0.86         3\n",
      "                  SHARE LISTINGS       1.00      0.50      0.67        12\n",
      "                          SPORTS       0.96      0.93      0.95        58\n",
      "\n",
      "                       micro avg       0.75      0.75      0.75       500\n",
      "                       macro avg       0.69      0.62      0.64       500\n",
      "                    weighted avg       0.75      0.75      0.75       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# choose solver='lbfgs',  N-Gram Vectors TF-IDF\n",
    "predicted_y = lg2.predict(xtest_tfidf_ngram)\n",
    "target = ['ARTS CULTURE ENTERTAINMENT',\n",
    "    'BIOGRAPHIES PERSONALITIES PEOPLE',\n",
    "    'DEFENCE',\n",
    "    'DOMESTIC MARKETS',\n",
    "    'FOREX MARKETS',\n",
    "    'HEALTH',\n",
    "    'RRELEVANT',\n",
    "    'MONEY MARKETS',\n",
    "    'SCIENCE AND TECHNOLOGY',\n",
    "    'SHARE LISTINGS',\n",
    "    'SPORTS'\n",
    "    ]\n",
    "\n",
    "print(classification_report(y_test, predicted_y,target_names=target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "      ARTS CULTURE ENTERTAINMENT       0.00      0.00      0.00         3\n",
      "BIOGRAPHIES PERSONALITIES PEOPLE       0.00      0.00      0.00        15\n",
      "                         DEFENCE       0.00      0.00      0.00        13\n",
      "                DOMESTIC MARKETS       0.00      0.00      0.00         2\n",
      "                   FOREX MARKETS       0.00      0.00      0.00        48\n",
      "                          HEALTH       0.00      0.00      0.00        14\n",
      "                       RRELEVANT       0.55      0.99      0.70       266\n",
      "                   MONEY MARKETS       0.00      0.00      0.00        69\n",
      "          SCIENCE AND TECHNOLOGY       0.00      0.00      0.00         3\n",
      "                  SHARE LISTINGS       0.00      0.00      0.00         7\n",
      "                          SPORTS       0.64      0.12      0.20        60\n",
      "\n",
      "                       micro avg       0.54      0.54      0.54       500\n",
      "                       macro avg       0.11      0.10      0.08       500\n",
      "                    weighted avg       0.37      0.54      0.40       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('test.csv')\n",
    "test_set = pd.DataFrame(columns=['words', 'topic'])\n",
    "x = df_test['article_words'].str.replace(',',' ')\n",
    "y = df_test['topic']\n",
    "\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word',token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(x)\n",
    "x_ngram = tfidf_vect_ngram.transform(x)\n",
    "\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "\n",
    "predicted_y = lg2.predict(x_ngram)\n",
    "print(classification_report(y, predicted_y,target_names=target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
